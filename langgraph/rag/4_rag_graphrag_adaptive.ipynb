{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DASHSCOPE_API_KEY'] = 'sk-a31a563bce7146bf8c8ab93ad5fea537'\n",
    "os.environ['COHERE_API_KEY'] = '5HPQX7dn7kV61LBb3EsQcfiqjtGbAPvk08bA4VkD'\n",
    "os.environ['TAVILY_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./embeddings/bge-base-zh-v1.5. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "embd = HuggingFaceBgeEmbeddings(\n",
    "    model_name='./embeddings/bge-base-zh-v1.5',\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=20\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents=docs_list)\n",
    "\n",
    "vectorestore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name='rag-chroma',\n",
    "    embedding=embd\n",
    ")\n",
    "retriever = vectorestore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n",
      "datasource='vectorestore'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal['vectorestore', 'web_search'] = Field(\n",
    "        ...,\n",
    "        description=\"给定用户问题，选择将其路由到网络搜索(web_search)或向量存储库(vectorstore)。\"\n",
    "    )\n",
    "\n",
    "llm = ChatTongyi()\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"你是用户问题 路由到 向量存储库或网络搜索 的专家。\n",
    "向量存储库包含与代理、提示工程和对抗攻击相关的文档。\n",
    "对于这些主题的问题，请使用向量存储库(vectorstore)。否则，请使用网络搜索(web_search)。\n",
    "\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(\n",
    "    question_router.invoke(\n",
    "        {\"question\": \"熊队会在 NFL 选秀中首轮选中哪位球员？\"}\n",
    "    )\n",
    ")\n",
    "print(question_router.invoke({\"question\": \"代理内存的类型有哪些？\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"用于检查检索文档相关性的二进制评分类。\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"检索文档与问题的相关性，yes 表示相关，no 表示不相关\"\n",
    "    )\n",
    "\n",
    "llm = ChatTongyi()\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = '''\n",
    "    你是一名评估员，需要评估检索到的文档与用户问题的相关性。\n",
    "    如果文档包含与用户问题相关的关键词或语义意义，请将其评为相关。\n",
    "    这不需要严格的测试，目标是过滤掉错误的检索结果。\n",
    "    使用二进制评分“yes”或“no”来表示文档是否与问题相关。\n",
    "'''\n",
    "grade_promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n{document}\\n, User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_promt | structured_llm_grader\n",
    "\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[2].page_content\n",
    "print(doc_txt)\n",
    "print(retrieval_grader.invoke({\"document\": doc_txt, \"question\": question}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zta/anaconda3/envs/langgraph/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLM-powered autonomous agents, memory is divided into short-term and long-term memory. Short-term memory involves in-context learning, while long-term memory allows agents to retain and recall information over extended periods using an external vector store for fast retrieval.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatTongyi()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chian = prompt | llm | StrOutputParser()\n",
    "generation = rag_chian.invoke({'context': docs, 'question': question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    '''生成的答案与文档相关性检查的二进制评分。'''\n",
    "    binary_score: str = Field(...,\n",
    "        description=\"答案是否基于检索到的文档，'yes'或'no'。\"\n",
    "    )\n",
    "\n",
    "llm = ChatTongyi()\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system = \"\"\"\n",
    "你是一个评估员，负责评估大型语言模型生成的答案是否 基于/得到 一组检索到的事实的支持。\n",
    "给出二进制评分“yes”或“no”。\n",
    "“yes”表示答案是基于/由这些事实支持的。\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "print(hallucination_grader.invoke({\"documents\": format_docs(docs), \"generation\": generation}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"判断回答是否与问题相匹配\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"判断是否回答了问题，yes表示回答了，no表示没有回答\"\n",
    "    )\n",
    "llm = ChatTongyi()\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "system = '''\n",
    "你是一个评估员，负责评估答案是否解决了问题。\n",
    "给出二进制评分“是”或“否”。\n",
    "“是”表示答案解决了问题。\n",
    "'''\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the role of memory in an agent's functioning?\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### question rewriter\n",
    "\n",
    "llm = ChatTongyi()\n",
    "\n",
    "system = '''\n",
    "你是一个问题重写者，负责将输入问题转化为更优化的版本，以便进行向量存储检索。\n",
    "查看输入问题，并尝试推理其潜在的语义意图/含义。\n",
    "你要做的工作是尝试重写用户所提出问题，将问题转换为更优化的版本。\n",
    "仅返回优化后的问题版本即可，不要返回其他内容。\n",
    "例如：\n",
    "问题：agent memory\n",
    "优化后：What is the role of memory in an agent's functioning?\n",
    "'''\n",
    "question = 'agent memory'\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
