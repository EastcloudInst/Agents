{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8647336-8765-4be2-a64d-0446ee983d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e096f2c3-16e1-4c24-8914-b6e08a0a7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='你好！当然可以。量子计算是一种基于量子力学原理的新型计算方式，它利用了量子比特（qubit）来存储和处理信息。与经典计算机中的比特不同，量子比特不仅可以表示0或1，还可以同时处于0和1的状态，这种现象被称为叠加态。此外，量子比特之间还可能存在纠缠关系，即使相隔很远也能瞬间影响彼此的状态。\\n\\n### 量子计算的基本概念\\n\\n- **量子比特**：是量子计算中最基本的信息单位。不同于传统二进制位只能取0或1值，量子比特能够同时表示这两种状态。\\n- **叠加态**：允许一个量子系统同时存在于多种可能的状态中，直到被观测时才会“坍缩”成一种确定状态。\\n- **量子纠缠**：两个或多个粒子间形成的一种特殊联系，使得无论它们相距多远，对其中一个粒子的操作都会立刻影响到另一个粒子的状态。\\n- **量子门**：用于操作量子比特的一系列基本运算单元，类似于经典逻辑门在普通计算机中的作用。\\n\\n### 量子计算的优势\\n\\n量子计算机能够在某些特定问题上提供指数级加速，比如大整数分解、优化问题求解以及模拟量子物理系统等。这主要得益于其独特的性质如并行性和纠缠性所带来的强大计算能力。\\n\\n### 面临挑战\\n\\n尽管前景广阔，但目前量子计算技术仍面临诸多挑战：\\n- **错误率高**：由于环境噪声等因素的影响，当前的量子比特非常容易出错。\\n- **可扩展性难题**：构建大规模可靠的量子计算机需要克服材料科学、工程设计等多个领域的难关。\\n- **算法开发**：虽然理论上已证明了一些问题可以通过量子算法高效解决，但实际上开发这些算法并将其应用于实际场景仍然充满挑战。\\n\\n总之，量子计算是一个极具潜力但也极其复杂的研究领域，正吸引着全世界科学家们的广泛关注与深入探索。随着相关技术的进步，未来或许能为我们打开一扇通往全新计算世界的大门。' additional_kwargs={} response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': 'cc1e85ac-d671-9eb1-941e-9a6a9fe47e5c', 'token_usage': {'input_tokens': 26, 'output_tokens': 410, 'total_tokens': 436}} id='run-98214d43-4c29-4c81-aa5b-771c7f6f527a-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "\n",
    "# 使用 Qwen 模型\n",
    "qwen_llm = ChatTongyi(\n",
    "    model=\"qwen-max\",              # Qwen 模型名称，例如 \"qwen-7b\" 或 \"qwen-14b\"\n",
    "    api_key=\"sk-a31a563bce7146bf8c8ab93ad5fea537\"   # 替换为实际的 API 密钥\n",
    ")\n",
    "\n",
    "# 调用 Qwen 模型\n",
    "response = qwen_llm.invoke(\"你好！能介绍一下量子计算吗？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b2b853-615e-4b3d-a33c-f0a385aafcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "import getpass \n",
    "import os \n",
    "\n",
    "# def _set_env(var: str): \n",
    "#     if not os.environ.get(var): \n",
    "#         os.environ[var] = getpass.getpass(f\"{var}: \") \n",
    "\n",
    "# # api key: sk-a31a563bce7146bf8c8ab93ad5fea537\n",
    "# _set_env(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "# llm = Tongyi()\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [qwen_llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80e6c61e-4bc0-446d-9834-c8996f2ac68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9330d1b-58a6-4eca-abfd-18cdd4978298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "def invoke_graph_(user_input: str):\n",
    "    answers = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
    "    print(answers)\n",
    "\n",
    "while True:\n",
    "    # try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # stream_graph_updates(user_input)\n",
    "        invoke_graph_(user_input)\n",
    "    \n",
    "    # except:\n",
    "    #     # fallback if input() is not available\n",
    "    #     user_input = \"What do you know about LangGraph?\"\n",
    "    #     print(\"User: \" + user_input)\n",
    "    #     stream_graph_updates(user_input)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c53217d-7edc-4d7c-98fa-42e2e3f4465e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['456']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StateTest(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "stateTest: StateTest = {\n",
    "    'messages': ['123']\n",
    "}\n",
    "\n",
    "stateTest['messages'] = ['456']\n",
    "\n",
    "stateTest['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de27e2fe-3ee6-4207-8a37-efdf1f973786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': [0.5, 0.75]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': [0.5, 0.75]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "def reducer(a: list, b: int | None) -> list:\n",
    "    if b is not None:\n",
    "        return a + [b]\n",
    "    return a\n",
    "\n",
    "class State(TypedDict):\n",
    "    x: Annotated[list, reducer]\n",
    "\n",
    "class ConfigSchema(TypedDict):\n",
    "    r: float\n",
    "\n",
    "graph = StateGraph(State, config_schema=ConfigSchema)\n",
    "\n",
    "def node(state: State, config: RunnableConfig) -> dict:\n",
    "    r = config[\"configurable\"].get(\"r\", 1.0)\n",
    "    x = state[\"x\"][-1]\n",
    "    next_value = x * r * (1 - x)\n",
    "    return {\"x\": next_value}\n",
    "\n",
    "graph.add_node(\"A\", node)\n",
    "graph.set_entry_point(\"A\")\n",
    "graph.set_finish_point(\"A\")\n",
    "compiled = graph.compile(checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "\n",
    "step1 = compiled.invoke({\"x\": 0.5}, {\"configurable\": {\"r\": 3.0, \"thread_id\": \"1\"}})\n",
    "print(step1)\n",
    "{'x': [0.5, 0.75]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a452b293-cda5-4ef6-a646-cc80f60f26bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': [0.5, 0.75, 0.6, 0.72]}\n"
     ]
    }
   ],
   "source": [
    "step1 = compiled.invoke({\"x\": 0.6}, {\"configurable\": {\"r\": 3.0, \"thread_id\": \"1\"}})\n",
    "print(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f994880-80b2-40b0-84d2-394e9f751c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if messages := []:\n",
    "    print(\"[]\")\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b0ad25-ca17-4a5a-8819-243f98fcc2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'function': {'name': 'GetMoney', 'arguments': '{\"Money\": \"San Francisco, CA\"}'}, 'index': 0, 'id': 'call_65338a678da545f3a03ae5', 'type': 'function'}]}, response_metadata={'model_name': 'qwen-max', 'finish_reason': 'tool_calls', 'request_id': '76eca852-ad9d-9773-b55b-d139d8cceb6c', 'token_usage': {'input_tokens': 252, 'output_tokens': 20, 'total_tokens': 272}}, id='run-21383236-5056-4a55-a1f0-586de827fb29-0', tool_calls=[{'name': 'GetMoney', 'args': {'Money': 'San Francisco, CA'}, 'id': 'call_65338a678da545f3a03ae5', 'type': 'tool_call'}])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    '''Get the current weather in a given location'''\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "class GetPrice(BaseModel):\n",
    "    '''Get the price of a specific product.'''\n",
    "\n",
    "    product: str = Field(..., description=\"The product to look up.\")\n",
    "\n",
    "\n",
    "# 使用 Qwen 模型\n",
    "llm = ChatTongyi(\n",
    "    model=\"qwen-max\",              # Qwen 模型名称，例如 \"qwen-7b\" 或 \"qwen-14b\"\n",
    "    api_key=\"sk-a31a563bce7146bf8c8ab93ad5fea537\"   # 替换为实际的 API 密钥\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([GetWeather, GetPrice])\n",
    "llm_with_tools.invoke(\"what is the weather like in San Francisco\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38920ade-f1bf-4c7f-873b-6b57f33dd7e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca3d5d-8054-47c9-be09-1652d9b0fd85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
